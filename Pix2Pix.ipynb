{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, pickle\n",
    "from lib import networks, utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.backends.cudnn.enabled:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input channel for generator\n",
    "in_ngc=3\n",
    "#output channel for generator\n",
    "out_ngc=3\n",
    "#input channel for discriminator\n",
    "in_ndc=6\n",
    "#output channel for discriminator\n",
    "out_ndc=1\n",
    "\n",
    "batch_size=1\n",
    "#number of filters in the first layer of generator\n",
    "ngf=64\n",
    "#number of filters in the first layer of discriminator\n",
    "ndf=32\n",
    "#the number of resnet block layer for generator\n",
    "# nb=9\n",
    "#input size\n",
    "input_size=64\n",
    "train_epoch=300\n",
    "\n",
    "#Discriminator learning rate, default=0.0002\n",
    "lrD=0.0002\n",
    "#Generator learning rate, default=0.0002\n",
    "lrG=0.0002\n",
    "#lambda for content loss\n",
    "con_lambda=5\n",
    "#beta1 for Adam optimizer\n",
    "beta1=0.5\n",
    "#beta2 for Adam optimizer\n",
    "beta2=0.999\n",
    "\n",
    "# n_downsampling = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'pix2pix_eyes_6'\n",
    "result_path = project_name+'_results'\n",
    "data_name = 'data/combine'\n",
    "# data_name = 'data/combine_w_blur_curated'\n",
    "test_data_name = 'data/eye_test'\n",
    "# results save path\n",
    "if not os.path.isdir(result_path):\n",
    "    os.makedirs(result_path)\n",
    "#ensure data folder exists\n",
    "if not os.path.isdir(data_name):\n",
    "    os.makedirs(data_name)\n",
    "    print(\"data folder does not exist!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader\n",
    "train_transform = transforms.Compose([\n",
    "        transforms.Resize((input_size, 3*input_size)),\n",
    "        transforms.ColorJitter(0.1,0.1,0.1,0.1),\n",
    "#         transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "        transforms.Resize((input_size, input_size)),\n",
    "        transforms.ColorJitter(0.1,0.1,0.1,0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# train_loader = utils.data_load(data_name, 'train', train_transform, batch_size, shuffle=False, drop_last=True)\n",
    "# test_loader = utils.data_load(data_name, 'test', train_transform, batch_size, shuffle=False, drop_last=True)\n",
    "train_loader = torch.utils.data.DataLoader(datasets.ImageFolder(data_name, train_transform), batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(datasets.ImageFolder(test_data_name, test_transform), batch_size=1, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Networks initialized -------------\n",
      "G has 29244035 number of parameters\n",
      "D has 1129249 number of parameters\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# network\n",
    "\n",
    "# G = networks.generator(in_ngc, out_ngc, ngf, nb)\n",
    "G = networks.UnetGenerator(in_ngc, out_ngc, 6, ngf)\n",
    "D = networks.discriminator(in_ndc, out_ndc, ndf)\n",
    "# D = networks.wgan_discriminator(in_ndc, ndf, input_size, n_downsampling)\n",
    "\n",
    "G.to(device)\n",
    "D.to(device)\n",
    "G.train()\n",
    "D.train();\n",
    "print('---------- Networks initialized -------------')\n",
    "for name,model in [('G',G),('D',D)]:\n",
    "    num_params = 0\n",
    "    for param in model.parameters():\n",
    "        num_params += param.numel()\n",
    "    print(str.format('{} has {} number of parameters', name, num_params))\n",
    "print('-----------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "# GAN_loss = nn.BCELoss().to(device)\n",
    "GAN_loss = nn.MSELoss().to(device)\n",
    "L1_loss = nn.L1Loss().to(device)\n",
    "# def D_loss_criterion(D_decision):\n",
    "#     return D_decision.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=lrG, betas=(beta1, beta2))\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=lrD, betas=(beta1, beta2))\n",
    "G_scheduler = optim.lr_scheduler.MultiStepLR(optimizer=G_optimizer, milestones=[train_epoch // 2, train_epoch // 4 * 3], gamma=0.1)\n",
    "D_scheduler = optim.lr_scheduler.MultiStepLR(optimizer=D_optimizer, milestones=[train_epoch // 2, train_epoch // 4 * 3], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hist = {}\n",
    "train_hist['Disc_loss'] = []\n",
    "train_hist['Gen_loss'] = []\n",
    "train_hist['Con_loss'] = []\n",
    "train_hist['per_epoch_time'] = []\n",
    "train_hist['Gen_loss_one_epoch']=[]\n",
    "train_hist['Disc_loss_one_epoch']=[]\n",
    "train_hist['Con_loss_one_epoch']=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starting_epoch is used to avoid overriding of the previously generated results\n",
    "starting_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start!\n",
      "[1/300] - time: 7.79, Disc loss: 0.148, Gen loss: 0.444, Con loss: 2.384\n",
      "[2/300] - time: 7.50, Disc loss: 0.169, Gen loss: 0.441, Con loss: 2.530\n",
      "[3/300] - time: 7.50, Disc loss: 0.153, Gen loss: 0.476, Con loss: 2.531\n",
      "[4/300] - time: 7.50, Disc loss: 0.158, Gen loss: 0.486, Con loss: 2.545\n",
      "[5/300] - time: 7.50, Disc loss: 0.149, Gen loss: 0.514, Con loss: 2.540\n",
      "[6/300] - time: 7.50, Disc loss: 0.152, Gen loss: 0.502, Con loss: 2.487\n",
      "[7/300] - time: 7.50, Disc loss: 0.141, Gen loss: 0.533, Con loss: 2.472\n",
      "[8/300] - time: 7.50, Disc loss: 0.138, Gen loss: 0.537, Con loss: 2.468\n",
      "[9/300] - time: 7.50, Disc loss: 0.139, Gen loss: 0.551, Con loss: 2.408\n",
      "[10/300] - time: 7.49, Disc loss: 0.132, Gen loss: 0.509, Con loss: 2.410\n",
      "[11/300] - time: 7.50, Disc loss: 0.128, Gen loss: 0.552, Con loss: 2.339\n",
      "[12/300] - time: 7.50, Disc loss: 0.136, Gen loss: 0.512, Con loss: 2.307\n",
      "[13/300] - time: 7.50, Disc loss: 0.138, Gen loss: 0.534, Con loss: 2.260\n",
      "[14/300] - time: 7.50, Disc loss: 0.126, Gen loss: 0.548, Con loss: 2.242\n",
      "[15/300] - time: 7.50, Disc loss: 0.120, Gen loss: 0.580, Con loss: 2.221\n",
      "[16/300] - time: 7.50, Disc loss: 0.133, Gen loss: 0.575, Con loss: 2.168\n",
      "[17/300] - time: 7.50, Disc loss: 0.109, Gen loss: 0.579, Con loss: 2.172\n",
      "[18/300] - time: 7.50, Disc loss: 0.115, Gen loss: 0.590, Con loss: 2.121\n",
      "[19/300] - time: 7.50, Disc loss: 0.128, Gen loss: 0.539, Con loss: 2.087\n",
      "[20/300] - time: 7.50, Disc loss: 0.112, Gen loss: 0.580, Con loss: 2.060\n",
      "[21/300] - time: 7.50, Disc loss: 0.109, Gen loss: 0.599, Con loss: 2.038\n",
      "[22/300] - time: 7.50, Disc loss: 0.109, Gen loss: 0.626, Con loss: 2.007\n",
      "[23/300] - time: 7.50, Disc loss: 0.116, Gen loss: 0.615, Con loss: 1.979\n",
      "[24/300] - time: 7.50, Disc loss: 0.112, Gen loss: 0.611, Con loss: 1.950\n",
      "[25/300] - time: 7.50, Disc loss: 0.113, Gen loss: 0.591, Con loss: 1.947\n",
      "[26/300] - time: 7.50, Disc loss: 0.122, Gen loss: 0.576, Con loss: 1.899\n",
      "[27/300] - time: 7.50, Disc loss: 0.110, Gen loss: 0.602, Con loss: 1.890\n",
      "[28/300] - time: 7.64, Disc loss: 0.128, Gen loss: 0.593, Con loss: 1.844\n",
      "[29/300] - time: 7.75, Disc loss: 0.125, Gen loss: 0.577, Con loss: 1.844\n",
      "[30/300] - time: 7.50, Disc loss: 0.120, Gen loss: 0.607, Con loss: 1.784\n",
      "[31/300] - time: 7.52, Disc loss: 0.119, Gen loss: 0.586, Con loss: 1.782\n",
      "[32/300] - time: 7.65, Disc loss: 0.124, Gen loss: 0.605, Con loss: 1.762\n",
      "[33/300] - time: 7.50, Disc loss: 0.133, Gen loss: 0.537, Con loss: 1.745\n",
      "[34/300] - time: 7.50, Disc loss: 0.126, Gen loss: 0.580, Con loss: 1.750\n",
      "[35/300] - time: 7.50, Disc loss: 0.129, Gen loss: 0.571, Con loss: 1.718\n",
      "[36/300] - time: 7.50, Disc loss: 0.131, Gen loss: 0.568, Con loss: 1.684\n",
      "[37/300] - time: 7.50, Disc loss: 0.117, Gen loss: 0.575, Con loss: 1.691\n",
      "[38/300] - time: 7.50, Disc loss: 0.136, Gen loss: 0.576, Con loss: 1.681\n",
      "[39/300] - time: 7.50, Disc loss: 0.126, Gen loss: 0.560, Con loss: 1.674\n",
      "[40/300] - time: 7.50, Disc loss: 0.125, Gen loss: 0.570, Con loss: 1.641\n",
      "[41/300] - time: 7.50, Disc loss: 0.125, Gen loss: 0.599, Con loss: 1.657\n",
      "[42/300] - time: 7.50, Disc loss: 0.118, Gen loss: 0.598, Con loss: 1.651\n",
      "[43/300] - time: 7.50, Disc loss: 0.127, Gen loss: 0.556, Con loss: 1.627\n",
      "[44/300] - time: 7.50, Disc loss: 0.118, Gen loss: 0.577, Con loss: 1.622\n",
      "[45/300] - time: 7.50, Disc loss: 0.122, Gen loss: 0.596, Con loss: 1.610\n",
      "[46/300] - time: 7.50, Disc loss: 0.124, Gen loss: 0.584, Con loss: 1.601\n",
      "[47/300] - time: 7.50, Disc loss: 0.122, Gen loss: 0.594, Con loss: 1.601\n",
      "[48/300] - time: 7.50, Disc loss: 0.121, Gen loss: 0.580, Con loss: 1.584\n",
      "[49/300] - time: 7.50, Disc loss: 0.127, Gen loss: 0.583, Con loss: 1.576\n",
      "[50/300] - time: 7.49, Disc loss: 0.124, Gen loss: 0.598, Con loss: 1.574\n",
      "[51/300] - time: 7.50, Disc loss: 0.125, Gen loss: 0.613, Con loss: 1.567\n",
      "[52/300] - time: 7.49, Disc loss: 0.128, Gen loss: 0.582, Con loss: 1.549\n",
      "[53/300] - time: 7.50, Disc loss: 0.124, Gen loss: 0.606, Con loss: 1.583\n",
      "[54/300] - time: 7.50, Disc loss: 0.132, Gen loss: 0.584, Con loss: 1.548\n",
      "[55/300] - time: 7.49, Disc loss: 0.132, Gen loss: 0.573, Con loss: 1.546\n",
      "[56/300] - time: 7.50, Disc loss: 0.122, Gen loss: 0.598, Con loss: 1.533\n",
      "[57/300] - time: 7.50, Disc loss: 0.130, Gen loss: 0.560, Con loss: 1.503\n",
      "[58/300] - time: 7.50, Disc loss: 0.128, Gen loss: 0.576, Con loss: 1.519\n",
      "[59/300] - time: 7.50, Disc loss: 0.137, Gen loss: 0.572, Con loss: 1.522\n",
      "[60/300] - time: 7.50, Disc loss: 0.135, Gen loss: 0.571, Con loss: 1.508\n",
      "[61/300] - time: 7.50, Disc loss: 0.134, Gen loss: 0.544, Con loss: 1.518\n",
      "[62/300] - time: 7.50, Disc loss: 0.132, Gen loss: 0.569, Con loss: 1.500\n",
      "[63/300] - time: 7.50, Disc loss: 0.128, Gen loss: 0.572, Con loss: 1.478\n",
      "[64/300] - time: 7.50, Disc loss: 0.138, Gen loss: 0.547, Con loss: 1.478\n",
      "[65/300] - time: 7.50, Disc loss: 0.135, Gen loss: 0.556, Con loss: 1.482\n",
      "[66/300] - time: 7.50, Disc loss: 0.143, Gen loss: 0.543, Con loss: 1.477\n",
      "[67/300] - time: 7.50, Disc loss: 0.137, Gen loss: 0.557, Con loss: 1.480\n",
      "[68/300] - time: 7.50, Disc loss: 0.141, Gen loss: 0.537, Con loss: 1.473\n",
      "[69/300] - time: 7.58, Disc loss: 0.143, Gen loss: 0.538, Con loss: 1.456\n",
      "[70/300] - time: 7.80, Disc loss: 0.146, Gen loss: 0.519, Con loss: 1.449\n",
      "[71/300] - time: 7.52, Disc loss: 0.140, Gen loss: 0.533, Con loss: 1.457\n",
      "[72/300] - time: 7.65, Disc loss: 0.139, Gen loss: 0.535, Con loss: 1.455\n",
      "[73/300] - time: 7.53, Disc loss: 0.145, Gen loss: 0.508, Con loss: 1.443\n",
      "[74/300] - time: 7.50, Disc loss: 0.140, Gen loss: 0.537, Con loss: 1.441\n",
      "[75/300] - time: 7.50, Disc loss: 0.142, Gen loss: 0.546, Con loss: 1.439\n",
      "[76/300] - time: 7.50, Disc loss: 0.140, Gen loss: 0.535, Con loss: 1.428\n",
      "[77/300] - time: 7.50, Disc loss: 0.141, Gen loss: 0.537, Con loss: 1.419\n",
      "[78/300] - time: 7.50, Disc loss: 0.137, Gen loss: 0.521, Con loss: 1.433\n",
      "[79/300] - time: 7.49, Disc loss: 0.140, Gen loss: 0.538, Con loss: 1.412\n",
      "[80/300] - time: 7.50, Disc loss: 0.143, Gen loss: 0.540, Con loss: 1.401\n",
      "[81/300] - time: 7.50, Disc loss: 0.142, Gen loss: 0.546, Con loss: 1.422\n",
      "[82/300] - time: 7.49, Disc loss: 0.139, Gen loss: 0.540, Con loss: 1.410\n",
      "[83/300] - time: 7.50, Disc loss: 0.146, Gen loss: 0.518, Con loss: 1.407\n",
      "[84/300] - time: 7.49, Disc loss: 0.141, Gen loss: 0.534, Con loss: 1.415\n",
      "[85/300] - time: 7.51, Disc loss: 0.139, Gen loss: 0.547, Con loss: 1.422\n",
      "[86/300] - time: 7.50, Disc loss: 0.140, Gen loss: 0.519, Con loss: 1.414\n",
      "[87/300] - time: 7.50, Disc loss: 0.140, Gen loss: 0.541, Con loss: 1.404\n",
      "[88/300] - time: 7.50, Disc loss: 0.141, Gen loss: 0.549, Con loss: 1.401\n",
      "[89/300] - time: 7.51, Disc loss: 0.135, Gen loss: 0.551, Con loss: 1.407\n",
      "[90/300] - time: 7.49, Disc loss: 0.137, Gen loss: 0.563, Con loss: 1.403\n",
      "[91/300] - time: 7.50, Disc loss: 0.137, Gen loss: 0.575, Con loss: 1.406\n",
      "[92/300] - time: 7.50, Disc loss: 0.140, Gen loss: 0.563, Con loss: 1.405\n",
      "[93/300] - time: 7.50, Disc loss: 0.131, Gen loss: 0.558, Con loss: 1.417\n",
      "[94/300] - time: 7.50, Disc loss: 0.131, Gen loss: 0.566, Con loss: 1.401\n",
      "[95/300] - time: 7.50, Disc loss: 0.131, Gen loss: 0.575, Con loss: 1.399\n",
      "[96/300] - time: 7.50, Disc loss: 0.135, Gen loss: 0.575, Con loss: 1.389\n",
      "[97/300] - time: 7.50, Disc loss: 0.134, Gen loss: 0.582, Con loss: 1.393\n",
      "[98/300] - time: 7.50, Disc loss: 0.124, Gen loss: 0.590, Con loss: 1.392\n",
      "[99/300] - time: 7.50, Disc loss: 0.131, Gen loss: 0.585, Con loss: 1.394\n",
      "[100/300] - time: 7.50, Disc loss: 0.126, Gen loss: 0.611, Con loss: 1.383\n",
      "[101/300] - time: 7.50, Disc loss: 0.121, Gen loss: 0.588, Con loss: 1.384\n",
      "[102/300] - time: 7.50, Disc loss: 0.127, Gen loss: 0.610, Con loss: 1.380\n",
      "[103/300] - time: 7.49, Disc loss: 0.125, Gen loss: 0.602, Con loss: 1.380\n",
      "[104/300] - time: 7.50, Disc loss: 0.121, Gen loss: 0.587, Con loss: 1.378\n",
      "[105/300] - time: 7.50, Disc loss: 0.126, Gen loss: 0.568, Con loss: 1.388\n",
      "[106/300] - time: 7.50, Disc loss: 0.115, Gen loss: 0.626, Con loss: 1.380\n",
      "[107/300] - time: 7.50, Disc loss: 0.133, Gen loss: 0.576, Con loss: 1.354\n",
      "[108/300] - time: 7.50, Disc loss: 0.124, Gen loss: 0.628, Con loss: 1.372\n",
      "[109/300] - time: 7.50, Disc loss: 0.117, Gen loss: 0.637, Con loss: 1.360\n",
      "[110/300] - time: 7.51, Disc loss: 0.106, Gen loss: 0.628, Con loss: 1.364\n",
      "[111/300] - time: 7.80, Disc loss: 0.111, Gen loss: 0.641, Con loss: 1.365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[112/300] - time: 7.58, Disc loss: 0.120, Gen loss: 0.612, Con loss: 1.376\n",
      "[113/300] - time: 7.68, Disc loss: 0.118, Gen loss: 0.638, Con loss: 1.357\n",
      "[114/300] - time: 7.49, Disc loss: 0.116, Gen loss: 0.624, Con loss: 1.374\n",
      "[115/300] - time: 7.51, Disc loss: 0.115, Gen loss: 0.644, Con loss: 1.348\n",
      "[116/300] - time: 7.50, Disc loss: 0.103, Gen loss: 0.673, Con loss: 1.362\n",
      "[117/300] - time: 7.50, Disc loss: 0.109, Gen loss: 0.672, Con loss: 1.364\n",
      "[118/300] - time: 7.50, Disc loss: 0.109, Gen loss: 0.661, Con loss: 1.353\n",
      "[119/300] - time: 7.50, Disc loss: 0.101, Gen loss: 0.656, Con loss: 1.360\n",
      "[120/300] - time: 7.50, Disc loss: 0.113, Gen loss: 0.641, Con loss: 1.358\n",
      "[121/300] - time: 7.50, Disc loss: 0.107, Gen loss: 0.657, Con loss: 1.355\n",
      "[122/300] - time: 7.50, Disc loss: 0.113, Gen loss: 0.670, Con loss: 1.345\n",
      "[123/300] - time: 7.50, Disc loss: 0.105, Gen loss: 0.659, Con loss: 1.351\n",
      "[124/300] - time: 7.51, Disc loss: 0.100, Gen loss: 0.663, Con loss: 1.360\n",
      "[125/300] - time: 7.50, Disc loss: 0.118, Gen loss: 0.643, Con loss: 1.335\n",
      "[126/300] - time: 7.50, Disc loss: 0.105, Gen loss: 0.646, Con loss: 1.339\n",
      "[127/300] - time: 7.50, Disc loss: 0.104, Gen loss: 0.655, Con loss: 1.337\n",
      "[128/300] - time: 7.50, Disc loss: 0.099, Gen loss: 0.685, Con loss: 1.351\n",
      "[129/300] - time: 7.50, Disc loss: 0.099, Gen loss: 0.670, Con loss: 1.350\n",
      "[130/300] - time: 7.50, Disc loss: 0.106, Gen loss: 0.677, Con loss: 1.335\n",
      "[131/300] - time: 7.50, Disc loss: 0.097, Gen loss: 0.671, Con loss: 1.346\n",
      "[132/300] - time: 7.50, Disc loss: 0.094, Gen loss: 0.686, Con loss: 1.327\n",
      "[133/300] - time: 7.51, Disc loss: 0.091, Gen loss: 0.700, Con loss: 1.339\n",
      "[134/300] - time: 7.50, Disc loss: 0.087, Gen loss: 0.729, Con loss: 1.348\n",
      "[135/300] - time: 7.50, Disc loss: 0.089, Gen loss: 0.697, Con loss: 1.347\n",
      "[136/300] - time: 7.50, Disc loss: 0.107, Gen loss: 0.696, Con loss: 1.336\n",
      "[137/300] - time: 7.50, Disc loss: 0.089, Gen loss: 0.710, Con loss: 1.329\n",
      "[138/300] - time: 7.51, Disc loss: 0.087, Gen loss: 0.719, Con loss: 1.333\n",
      "[139/300] - time: 7.50, Disc loss: 0.087, Gen loss: 0.731, Con loss: 1.322\n",
      "[140/300] - time: 7.50, Disc loss: 0.081, Gen loss: 0.730, Con loss: 1.348\n",
      "[141/300] - time: 7.51, Disc loss: 0.095, Gen loss: 0.719, Con loss: 1.331\n",
      "[142/300] - time: 7.50, Disc loss: 0.095, Gen loss: 0.700, Con loss: 1.336\n",
      "[143/300] - time: 7.51, Disc loss: 0.102, Gen loss: 0.694, Con loss: 1.342\n",
      "[144/300] - time: 7.51, Disc loss: 0.092, Gen loss: 0.724, Con loss: 1.335\n",
      "[145/300] - time: 7.50, Disc loss: 0.082, Gen loss: 0.734, Con loss: 1.340\n",
      "[146/300] - time: 7.50, Disc loss: 0.070, Gen loss: 0.769, Con loss: 1.328\n",
      "[147/300] - time: 7.51, Disc loss: 0.071, Gen loss: 0.778, Con loss: 1.334\n",
      "[148/300] - time: 7.51, Disc loss: 0.099, Gen loss: 0.734, Con loss: 1.326\n",
      "[149/300] - time: 7.51, Disc loss: 0.068, Gen loss: 0.764, Con loss: 1.327\n",
      "[150/300] - time: 7.51, Disc loss: 0.073, Gen loss: 0.779, Con loss: 1.319\n",
      "[151/300] - time: 7.51, Disc loss: 0.055, Gen loss: 0.722, Con loss: 1.315\n",
      "[152/300] - time: 7.75, Disc loss: 0.085, Gen loss: 0.633, Con loss: 1.299\n",
      "[153/300] - time: 7.78, Disc loss: 0.097, Gen loss: 0.600, Con loss: 1.282\n",
      "[154/300] - time: 7.55, Disc loss: 0.091, Gen loss: 0.615, Con loss: 1.309\n",
      "[155/300] - time: 7.51, Disc loss: 0.093, Gen loss: 0.606, Con loss: 1.293\n",
      "[156/300] - time: 7.51, Disc loss: 0.096, Gen loss: 0.603, Con loss: 1.296\n",
      "[157/300] - time: 7.51, Disc loss: 0.096, Gen loss: 0.596, Con loss: 1.293\n",
      "[158/300] - time: 7.51, Disc loss: 0.097, Gen loss: 0.605, Con loss: 1.290\n",
      "[159/300] - time: 7.50, Disc loss: 0.094, Gen loss: 0.602, Con loss: 1.300\n",
      "[160/300] - time: 7.50, Disc loss: 0.095, Gen loss: 0.607, Con loss: 1.293\n",
      "[161/300] - time: 7.51, Disc loss: 0.097, Gen loss: 0.599, Con loss: 1.294\n",
      "[162/300] - time: 7.50, Disc loss: 0.096, Gen loss: 0.592, Con loss: 1.297\n",
      "[163/300] - time: 7.50, Disc loss: 0.099, Gen loss: 0.601, Con loss: 1.298\n",
      "[164/300] - time: 7.51, Disc loss: 0.097, Gen loss: 0.590, Con loss: 1.311\n",
      "[165/300] - time: 7.50, Disc loss: 0.095, Gen loss: 0.593, Con loss: 1.310\n",
      "[166/300] - time: 7.51, Disc loss: 0.099, Gen loss: 0.592, Con loss: 1.288\n",
      "[167/300] - time: 7.50, Disc loss: 0.091, Gen loss: 0.604, Con loss: 1.317\n",
      "[168/300] - time: 7.50, Disc loss: 0.101, Gen loss: 0.590, Con loss: 1.294\n",
      "[169/300] - time: 7.51, Disc loss: 0.104, Gen loss: 0.579, Con loss: 1.306\n",
      "[170/300] - time: 7.51, Disc loss: 0.100, Gen loss: 0.585, Con loss: 1.314\n",
      "[171/300] - time: 7.50, Disc loss: 0.102, Gen loss: 0.585, Con loss: 1.322\n",
      "[172/300] - time: 7.50, Disc loss: 0.097, Gen loss: 0.585, Con loss: 1.332\n",
      "[173/300] - time: 7.51, Disc loss: 0.104, Gen loss: 0.573, Con loss: 1.318\n",
      "[174/300] - time: 7.51, Disc loss: 0.104, Gen loss: 0.569, Con loss: 1.322\n",
      "[175/300] - time: 7.50, Disc loss: 0.108, Gen loss: 0.573, Con loss: 1.308\n",
      "[176/300] - time: 7.50, Disc loss: 0.105, Gen loss: 0.569, Con loss: 1.314\n",
      "[177/300] - time: 7.51, Disc loss: 0.103, Gen loss: 0.568, Con loss: 1.310\n",
      "[178/300] - time: 7.52, Disc loss: 0.107, Gen loss: 0.563, Con loss: 1.320\n",
      "[179/300] - time: 7.51, Disc loss: 0.109, Gen loss: 0.561, Con loss: 1.304\n",
      "[180/300] - time: 7.51, Disc loss: 0.103, Gen loss: 0.575, Con loss: 1.332\n",
      "[181/300] - time: 7.51, Disc loss: 0.109, Gen loss: 0.558, Con loss: 1.313\n",
      "[182/300] - time: 7.50, Disc loss: 0.105, Gen loss: 0.559, Con loss: 1.323\n",
      "[183/300] - time: 7.50, Disc loss: 0.108, Gen loss: 0.570, Con loss: 1.329\n",
      "[184/300] - time: 7.50, Disc loss: 0.107, Gen loss: 0.563, Con loss: 1.320\n",
      "[185/300] - time: 7.50, Disc loss: 0.113, Gen loss: 0.561, Con loss: 1.318\n",
      "[186/300] - time: 7.49, Disc loss: 0.103, Gen loss: 0.562, Con loss: 1.323\n",
      "[187/300] - time: 7.50, Disc loss: 0.108, Gen loss: 0.566, Con loss: 1.327\n",
      "[188/300] - time: 7.50, Disc loss: 0.111, Gen loss: 0.566, Con loss: 1.336\n",
      "[189/300] - time: 7.50, Disc loss: 0.107, Gen loss: 0.557, Con loss: 1.341\n",
      "[190/300] - time: 7.50, Disc loss: 0.115, Gen loss: 0.556, Con loss: 1.325\n",
      "[191/300] - time: 7.49, Disc loss: 0.112, Gen loss: 0.558, Con loss: 1.327\n",
      "[192/300] - time: 7.49, Disc loss: 0.109, Gen loss: 0.567, Con loss: 1.335\n",
      "[193/300] - time: 7.69, Disc loss: 0.111, Gen loss: 0.561, Con loss: 1.334\n",
      "[194/300] - time: 7.73, Disc loss: 0.110, Gen loss: 0.557, Con loss: 1.328\n",
      "[195/300] - time: 7.50, Disc loss: 0.117, Gen loss: 0.551, Con loss: 1.327\n",
      "[196/300] - time: 7.49, Disc loss: 0.121, Gen loss: 0.554, Con loss: 1.333\n",
      "[197/300] - time: 7.50, Disc loss: 0.115, Gen loss: 0.556, Con loss: 1.333\n",
      "[198/300] - time: 7.50, Disc loss: 0.115, Gen loss: 0.551, Con loss: 1.327\n",
      "[199/300] - time: 7.50, Disc loss: 0.112, Gen loss: 0.556, Con loss: 1.335\n",
      "[200/300] - time: 7.50, Disc loss: 0.113, Gen loss: 0.554, Con loss: 1.347\n",
      "[201/300] - time: 7.50, Disc loss: 0.112, Gen loss: 0.557, Con loss: 1.344\n",
      "[202/300] - time: 7.49, Disc loss: 0.111, Gen loss: 0.556, Con loss: 1.346\n",
      "[203/300] - time: 7.50, Disc loss: 0.115, Gen loss: 0.558, Con loss: 1.336\n",
      "[204/300] - time: 7.50, Disc loss: 0.114, Gen loss: 0.558, Con loss: 1.341\n",
      "[205/300] - time: 7.49, Disc loss: 0.109, Gen loss: 0.554, Con loss: 1.349\n",
      "[206/300] - time: 7.51, Disc loss: 0.114, Gen loss: 0.554, Con loss: 1.338\n",
      "[207/300] - time: 7.50, Disc loss: 0.112, Gen loss: 0.550, Con loss: 1.340\n",
      "[208/300] - time: 7.50, Disc loss: 0.119, Gen loss: 0.553, Con loss: 1.344\n",
      "[209/300] - time: 7.50, Disc loss: 0.117, Gen loss: 0.543, Con loss: 1.338\n",
      "[210/300] - time: 7.52, Disc loss: 0.122, Gen loss: 0.536, Con loss: 1.327\n",
      "[211/300] - time: 7.50, Disc loss: 0.119, Gen loss: 0.554, Con loss: 1.340\n",
      "[212/300] - time: 7.50, Disc loss: 0.119, Gen loss: 0.541, Con loss: 1.331\n",
      "[213/300] - time: 7.51, Disc loss: 0.116, Gen loss: 0.540, Con loss: 1.351\n",
      "[214/300] - time: 7.50, Disc loss: 0.112, Gen loss: 0.552, Con loss: 1.345\n",
      "[215/300] - time: 7.51, Disc loss: 0.118, Gen loss: 0.547, Con loss: 1.348\n",
      "[216/300] - time: 7.51, Disc loss: 0.115, Gen loss: 0.555, Con loss: 1.345\n",
      "[217/300] - time: 7.51, Disc loss: 0.117, Gen loss: 0.547, Con loss: 1.339\n",
      "[218/300] - time: 7.50, Disc loss: 0.121, Gen loss: 0.540, Con loss: 1.335\n",
      "[219/300] - time: 7.51, Disc loss: 0.119, Gen loss: 0.535, Con loss: 1.344\n",
      "[220/300] - time: 7.50, Disc loss: 0.118, Gen loss: 0.552, Con loss: 1.345\n",
      "[221/300] - time: 7.50, Disc loss: 0.117, Gen loss: 0.537, Con loss: 1.343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[222/300] - time: 7.50, Disc loss: 0.118, Gen loss: 0.550, Con loss: 1.349\n",
      "[223/300] - time: 7.50, Disc loss: 0.122, Gen loss: 0.539, Con loss: 1.359\n",
      "[224/300] - time: 7.50, Disc loss: 0.111, Gen loss: 0.553, Con loss: 1.348\n",
      "[225/300] - time: 7.51, Disc loss: 0.120, Gen loss: 0.547, Con loss: 1.343\n",
      "[226/300] - time: 7.51, Disc loss: 0.114, Gen loss: 0.528, Con loss: 1.338\n",
      "[227/300] - time: 7.50, Disc loss: 0.113, Gen loss: 0.536, Con loss: 1.332\n",
      "[228/300] - time: 7.51, Disc loss: 0.118, Gen loss: 0.532, Con loss: 1.338\n",
      "[229/300] - time: 7.51, Disc loss: 0.115, Gen loss: 0.537, Con loss: 1.343\n",
      "[230/300] - time: 7.51, Disc loss: 0.113, Gen loss: 0.532, Con loss: 1.346\n",
      "[231/300] - time: 7.51, Disc loss: 0.116, Gen loss: 0.536, Con loss: 1.330\n",
      "[232/300] - time: 7.51, Disc loss: 0.115, Gen loss: 0.535, Con loss: 1.337\n",
      "[233/300] - time: 7.50, Disc loss: 0.115, Gen loss: 0.537, Con loss: 1.343\n",
      "[234/300] - time: 7.64, Disc loss: 0.116, Gen loss: 0.533, Con loss: 1.342\n",
      "[235/300] - time: 7.78, Disc loss: 0.113, Gen loss: 0.539, Con loss: 1.338\n",
      "[236/300] - time: 7.50, Disc loss: 0.117, Gen loss: 0.523, Con loss: 1.352\n",
      "[237/300] - time: 7.51, Disc loss: 0.111, Gen loss: 0.539, Con loss: 1.338\n",
      "[238/300] - time: 7.50, Disc loss: 0.116, Gen loss: 0.546, Con loss: 1.353\n",
      "[239/300] - time: 7.50, Disc loss: 0.116, Gen loss: 0.534, Con loss: 1.346\n",
      "[240/300] - time: 7.50, Disc loss: 0.116, Gen loss: 0.532, Con loss: 1.344\n",
      "[241/300] - time: 7.50, Disc loss: 0.111, Gen loss: 0.547, Con loss: 1.351\n",
      "[242/300] - time: 7.51, Disc loss: 0.117, Gen loss: 0.525, Con loss: 1.331\n",
      "[243/300] - time: 7.52, Disc loss: 0.113, Gen loss: 0.542, Con loss: 1.351\n",
      "[244/300] - time: 7.51, Disc loss: 0.116, Gen loss: 0.550, Con loss: 1.342\n",
      "[245/300] - time: 7.51, Disc loss: 0.114, Gen loss: 0.530, Con loss: 1.351\n",
      "[246/300] - time: 7.51, Disc loss: 0.113, Gen loss: 0.534, Con loss: 1.348\n",
      "[247/300] - time: 7.52, Disc loss: 0.116, Gen loss: 0.529, Con loss: 1.345\n",
      "[248/300] - time: 7.51, Disc loss: 0.115, Gen loss: 0.540, Con loss: 1.345\n",
      "[249/300] - time: 7.52, Disc loss: 0.114, Gen loss: 0.531, Con loss: 1.350\n",
      "[250/300] - time: 7.51, Disc loss: 0.113, Gen loss: 0.543, Con loss: 1.355\n",
      "[251/300] - time: 7.52, Disc loss: 0.110, Gen loss: 0.540, Con loss: 1.343\n",
      "[252/300] - time: 7.51, Disc loss: 0.117, Gen loss: 0.536, Con loss: 1.348\n",
      "[253/300] - time: 7.52, Disc loss: 0.117, Gen loss: 0.536, Con loss: 1.336\n",
      "[254/300] - time: 7.52, Disc loss: 0.112, Gen loss: 0.537, Con loss: 1.334\n",
      "[255/300] - time: 7.51, Disc loss: 0.116, Gen loss: 0.534, Con loss: 1.341\n",
      "[256/300] - time: 7.52, Disc loss: 0.111, Gen loss: 0.540, Con loss: 1.352\n",
      "[257/300] - time: 7.51, Disc loss: 0.114, Gen loss: 0.542, Con loss: 1.354\n",
      "[258/300] - time: 7.51, Disc loss: 0.113, Gen loss: 0.539, Con loss: 1.340\n",
      "[259/300] - time: 7.51, Disc loss: 0.113, Gen loss: 0.535, Con loss: 1.341\n",
      "[260/300] - time: 7.51, Disc loss: 0.112, Gen loss: 0.544, Con loss: 1.347\n",
      "[261/300] - time: 7.50, Disc loss: 0.114, Gen loss: 0.542, Con loss: 1.347\n",
      "[262/300] - time: 7.50, Disc loss: 0.115, Gen loss: 0.534, Con loss: 1.346\n",
      "[263/300] - time: 7.51, Disc loss: 0.113, Gen loss: 0.532, Con loss: 1.347\n",
      "[264/300] - time: 7.51, Disc loss: 0.117, Gen loss: 0.537, Con loss: 1.353\n",
      "[265/300] - time: 7.51, Disc loss: 0.115, Gen loss: 0.538, Con loss: 1.341\n",
      "[266/300] - time: 7.51, Disc loss: 0.114, Gen loss: 0.535, Con loss: 1.350\n",
      "[267/300] - time: 7.50, Disc loss: 0.111, Gen loss: 0.537, Con loss: 1.357\n",
      "[268/300] - time: 7.51, Disc loss: 0.119, Gen loss: 0.534, Con loss: 1.348\n",
      "[269/300] - time: 7.50, Disc loss: 0.116, Gen loss: 0.533, Con loss: 1.340\n",
      "[270/300] - time: 7.50, Disc loss: 0.117, Gen loss: 0.532, Con loss: 1.339\n",
      "[271/300] - time: 7.50, Disc loss: 0.111, Gen loss: 0.545, Con loss: 1.355\n",
      "[272/300] - time: 7.51, Disc loss: 0.116, Gen loss: 0.537, Con loss: 1.356\n",
      "[273/300] - time: 7.51, Disc loss: 0.117, Gen loss: 0.537, Con loss: 1.348\n",
      "[274/300] - time: 7.51, Disc loss: 0.116, Gen loss: 0.530, Con loss: 1.337\n",
      "[275/300] - time: 7.76, Disc loss: 0.114, Gen loss: 0.539, Con loss: 1.334\n",
      "[276/300] - time: 7.81, Disc loss: 0.116, Gen loss: 0.530, Con loss: 1.353\n",
      "[277/300] - time: 7.55, Disc loss: 0.114, Gen loss: 0.537, Con loss: 1.352\n",
      "[278/300] - time: 7.51, Disc loss: 0.120, Gen loss: 0.522, Con loss: 1.342\n",
      "[279/300] - time: 7.51, Disc loss: 0.113, Gen loss: 0.537, Con loss: 1.355\n",
      "[280/300] - time: 7.55, Disc loss: 0.113, Gen loss: 0.538, Con loss: 1.350\n",
      "[281/300] - time: 7.51, Disc loss: 0.118, Gen loss: 0.533, Con loss: 1.352\n",
      "[282/300] - time: 7.51, Disc loss: 0.114, Gen loss: 0.530, Con loss: 1.343\n",
      "[283/300] - time: 7.52, Disc loss: 0.113, Gen loss: 0.533, Con loss: 1.348\n",
      "[284/300] - time: 7.51, Disc loss: 0.114, Gen loss: 0.536, Con loss: 1.344\n",
      "[285/300] - time: 7.51, Disc loss: 0.111, Gen loss: 0.535, Con loss: 1.345\n",
      "[286/300] - time: 7.51, Disc loss: 0.117, Gen loss: 0.533, Con loss: 1.345\n",
      "[287/300] - time: 7.51, Disc loss: 0.118, Gen loss: 0.536, Con loss: 1.354\n",
      "[288/300] - time: 7.51, Disc loss: 0.117, Gen loss: 0.530, Con loss: 1.337\n",
      "[289/300] - time: 7.51, Disc loss: 0.113, Gen loss: 0.533, Con loss: 1.352\n",
      "[290/300] - time: 7.52, Disc loss: 0.115, Gen loss: 0.540, Con loss: 1.365\n",
      "[291/300] - time: 7.51, Disc loss: 0.115, Gen loss: 0.537, Con loss: 1.349\n",
      "[292/300] - time: 7.51, Disc loss: 0.115, Gen loss: 0.531, Con loss: 1.334\n",
      "[293/300] - time: 7.51, Disc loss: 0.119, Gen loss: 0.534, Con loss: 1.345\n",
      "[294/300] - time: 7.51, Disc loss: 0.112, Gen loss: 0.531, Con loss: 1.350\n",
      "[295/300] - time: 7.51, Disc loss: 0.116, Gen loss: 0.540, Con loss: 1.348\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-22eafef23285>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mDisc_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDisc_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mtrain_hist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Disc_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDisc_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mDisc_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mD_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('training start!')\n",
    "start_time = time.time()\n",
    "num_pool = 50\n",
    "fake_pool = utils.ImagePool(num_pool)\n",
    "for epoch in range(train_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    G.train()\n",
    "    G_scheduler.step()\n",
    "    D_scheduler.step()\n",
    "    Disc_losses = []\n",
    "    Gen_losses = []\n",
    "    Con_losses = []\n",
    "    for d, _ in train_loader:\n",
    "        # x is the image at left, the source image\n",
    "        # y is the image at right, the target image\n",
    "        x = d[:, :, :, :input_size]\n",
    "        y = d[:, :, :, input_size:input_size*2]\n",
    "        e = d[:,:,:,input_size*2:]\n",
    "        x, y, e = x.to(device), y.to(device), e.to(device)\n",
    "        \n",
    "        # train D\n",
    "        for param in D.parameters():\n",
    "            param.requires_grad = True\n",
    "#             param.data.clamp_(-0.005,0.005)\n",
    "        D_optimizer.zero_grad()\n",
    "\n",
    "#         D_real = D(x)\n",
    "        real = torch.cat((y,x),1)\n",
    "        D_real = D(real)\n",
    "#         D_real_loss = D_loss_criterion(D_real)\n",
    "#         D_real_loss = GAN_loss(D_real, 1-torch.rand(D_real.size(),device = device)/10.0)\n",
    "        D_real_loss = GAN_loss(D_real, torch.ones(D_real.size(),device = device))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            G_ = G(x)\n",
    "        generated = torch.cat((G_,x), 1)\n",
    "        generated = fake_pool.query(generated.detach())\n",
    "        D_fake = D(generated)\n",
    "#         D_fake_loss = D_loss_criterion(D_fake)\n",
    "#         D_fake_loss = GAN_loss(D_fake, torch.rand(D_fake.size(),device = device)/10.0)\n",
    "        D_fake_loss = GAN_loss(D_fake, torch.zeros(D_fake.size(),device = device))\n",
    "        \n",
    "#         blurred = torch.cat((e,x),1)\n",
    "#         D_blur = D(e)\n",
    "#         D_blur_loss = GAN_loss(D_blur, torch.rand(D_blur.size(),device = device)/10.0) \n",
    "\n",
    "        Disc_loss = 0.5*(D_real_loss + D_fake_loss)\n",
    "#         Disc_loss = (D_real_loss + D_fake_loss + D_blur_loss)/3\n",
    "#         Disc_loss = D_fake_loss - D_real_loss\n",
    "        Disc_losses.append(Disc_loss.item())\n",
    "        train_hist['Disc_loss'].append(Disc_loss.item())\n",
    "        Disc_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "        # train G\n",
    "        for param in D.parameters():\n",
    "            param.requires_grad = False\n",
    "        G_optimizer.zero_grad()\n",
    "\n",
    "        G_ = G(x)\n",
    "        generated = torch.cat((G_,x), 1)\n",
    "        D_fake = D(generated)\n",
    "#         D_fake_loss = D_loss_criterion(D_fake)\n",
    "        D_fake_loss = GAN_loss(D_fake, torch.ones(D_fake.size(),device = device))\n",
    "\n",
    "        Con_loss = con_lambda * L1_loss(G_, y)\n",
    "\n",
    "        Gen_loss = D_fake_loss + Con_loss\n",
    "#         Gen_loss = -D_fake_loss + Con_loss\n",
    "\n",
    "        Gen_losses.append(D_fake_loss.item())\n",
    "        train_hist['Gen_loss'].append(D_fake_loss.item())\n",
    "        Con_losses.append(Con_loss.item())\n",
    "        train_hist['Con_loss'].append(Con_loss.item())\n",
    "\n",
    "        Gen_loss.backward()\n",
    "        G_optimizer.step()\n",
    "\n",
    "\n",
    "    per_epoch_time = time.time() - epoch_start_time\n",
    "    train_hist['per_epoch_time'].append(per_epoch_time)\n",
    "    \n",
    "    Gen_loss_avg = torch.mean(torch.FloatTensor(Gen_losses))\n",
    "    Con_loss_avg = torch.mean(torch.FloatTensor(Con_losses))\n",
    "    Disc_loss_avg =  torch.mean(torch.FloatTensor(Disc_losses))\n",
    "    \n",
    "    train_hist['Gen_loss_one_epoch'].append(Gen_loss_avg)\n",
    "    train_hist['Disc_loss_one_epoch'].append(Disc_loss_avg)\n",
    "    train_hist['Con_loss_one_epoch'].append(Con_loss_avg)\n",
    "    \n",
    "    print(\n",
    "    '[%d/%d] - time: %.2f, Disc loss: %.3f, Gen loss: %.3f, Con loss: %.3f' % ((starting_epoch + epoch + 1), train_epoch, per_epoch_time, Disc_loss_avg, Gen_loss_avg, Con_loss_avg))\n",
    "    \n",
    "    if(epoch%10 == 0):\n",
    "        with torch.no_grad():\n",
    "            G.eval()\n",
    "            for n, (d, _) in enumerate(train_loader):\n",
    "                x = d[:, :, :, :input_size]\n",
    "                y = d[:, :, :, input_size:input_size*2]\n",
    "                x,y = x.to(device),y.to(device)\n",
    "                G_recon = G(x)\n",
    "                result = torch.cat((x[0], G_recon[0],y[0]), 2)\n",
    "                path = os.path.join(result_path, str(starting_epoch+epoch+1) + '_epoch_' + project_name + '_train_' + str(n + 1) + '.png')\n",
    "                plt.imsave(path, (result.cpu().numpy().transpose(1, 2, 0) + 1) / 2)\n",
    "                if n == 2:\n",
    "                    break\n",
    "            for n, (d, _) in enumerate(test_loader):\n",
    "                d = d.to(device)\n",
    "                G_recon = G(d)\n",
    "                result = torch.cat((d[0], G_recon[0]), 2)\n",
    "                path = os.path.join(result_path, str(starting_epoch+epoch+1) + '_epoch_' + project_name + '_test_' + str(n + 1) + '.png')\n",
    "                plt.imsave(path, (result.cpu().numpy().transpose(1, 2, 0) + 1) / 2)\n",
    "                if n == 2:\n",
    "                    break\n",
    "    if(epoch%40 == 0):\n",
    "        torch.save(G.state_dict(), os.path.join(result_path, str(epoch)+'_generator_latest.pkl'))\n",
    "        torch.save(D.state_dict(), os.path.join(result_path, str(epoch)+'_discriminator_latest.pkl'))\n",
    "        with open(os.path.join(result_path,  'train_hist.pkl'), 'wb') as f:\n",
    "            pickle.dump(train_hist, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
