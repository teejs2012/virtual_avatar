{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, pickle\n",
    "from lib import networks, utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.backends.cudnn.enabled:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input channel for generator\n",
    "in_ngc=3\n",
    "#output channel for generator\n",
    "out_ngc=3\n",
    "#input channel for discriminator\n",
    "in_ndc=6\n",
    "#output channel for discriminator\n",
    "out_ndc=1\n",
    "\n",
    "batch_size=2\n",
    "#number of filters in the first layer of generator\n",
    "ngf=32\n",
    "#number of filters in the first layer of discriminator\n",
    "ndf=32\n",
    "#the number of resnet block layer for generator\n",
    "nb=9\n",
    "#input size\n",
    "input_size=128\n",
    "train_epoch=100\n",
    "\n",
    "#Discriminator learning rate, default=0.0002\n",
    "lrD=0.0002\n",
    "#Generator learning rate, default=0.0002\n",
    "lrG=0.0002\n",
    "#lambda for content loss\n",
    "con_lambda=10\n",
    "#beta1 for Adam optimizer\n",
    "beta1=0.5\n",
    "#beta2 for Adam optimizer\n",
    "beta2=0.999\n",
    "\n",
    "n_downsampling = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'pix2pix_facades_1'\n",
    "result_path = project_name+'_results'\n",
    "data_name = 'data/facades'\n",
    "\n",
    "# results save path\n",
    "if not os.path.isdir(result_path):\n",
    "    os.makedirs(result_path)\n",
    "#ensure data folder exists\n",
    "if not os.path.isdir(data_name):\n",
    "    os.makedirs(data_name)\n",
    "    print(\"data folder does not exist!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader\n",
    "train_transform = transforms.Compose([\n",
    "        transforms.Resize((input_size, 2*input_size)),\n",
    "        transforms.ColorJitter(0.1,0.1,0.1,0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_loader = utils.data_load(data_name, 'train', train_transform, batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = utils.data_load(data_name, 'test', train_transform, batch_size, shuffle=False, drop_last=True)\n",
    "# train_loader = torch.utils.data.DataLoader(datasets.ImageFolder(data_name, train_transform), batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network\n",
    "\n",
    "# G = networks.generator(in_ngc, out_ngc, ngf, nb)\n",
    "G = networks.UnetGenerator(in_ngc, out_ngc, 7, ngf)\n",
    "D = networks.discriminator(in_ndc, out_ndc, ndf)\n",
    "# D = networks.wgan_discriminator(in_ndc, ndf, input_size, n_downsampling)\n",
    "\n",
    "G.to(device)\n",
    "D.to(device)\n",
    "G.train()\n",
    "D.train();\n",
    "# print('---------- Networks initialized -------------')\n",
    "# utils.print_network(G)\n",
    "# utils.print_network(D)\n",
    "# print('-----------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "# GAN_loss = nn.BCELoss().to(device)\n",
    "GAN_loss = nn.MSELoss().to(device)\n",
    "L1_loss = nn.L1Loss().to(device)\n",
    "# def D_loss_criterion(D_decision):\n",
    "#     return D_decision.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=lrG, betas=(beta1, beta2))\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=lrD, betas=(beta1, beta2))\n",
    "G_scheduler = optim.lr_scheduler.MultiStepLR(optimizer=G_optimizer, milestones=[train_epoch // 2, train_epoch // 4 * 3], gamma=0.1)\n",
    "D_scheduler = optim.lr_scheduler.MultiStepLR(optimizer=D_optimizer, milestones=[train_epoch // 2, train_epoch // 4 * 3], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hist = {}\n",
    "train_hist['Disc_loss'] = []\n",
    "train_hist['Gen_loss'] = []\n",
    "train_hist['Con_loss'] = []\n",
    "train_hist['per_epoch_time'] = []\n",
    "train_hist['Gen_loss_one_epoch']=[]\n",
    "train_hist['Disc_loss_one_epoch']=[]\n",
    "train_hist['Con_loss_one_epoch']=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starting_epoch is used to avoid overriding of the previously generated results\n",
    "starting_epoch = 67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start!\n",
      "[68/100] - time: 6.95, Disc loss: 0.074, Gen loss: 0.640, Con loss: 1.609\n",
      "[69/100] - time: 6.84, Disc loss: 0.065, Gen loss: 0.612, Con loss: 1.593\n",
      "[70/100] - time: 6.83, Disc loss: 0.064, Gen loss: 0.624, Con loss: 1.600\n",
      "[71/100] - time: 7.34, Disc loss: 0.060, Gen loss: 0.640, Con loss: 1.580\n",
      "[72/100] - time: 7.40, Disc loss: 0.063, Gen loss: 0.625, Con loss: 1.565\n",
      "[73/100] - time: 6.85, Disc loss: 0.062, Gen loss: 0.628, Con loss: 1.562\n",
      "[74/100] - time: 6.84, Disc loss: 0.071, Gen loss: 0.616, Con loss: 1.562\n",
      "[75/100] - time: 6.83, Disc loss: 0.057, Gen loss: 0.632, Con loss: 1.564\n",
      "[76/100] - time: 6.85, Disc loss: 0.067, Gen loss: 0.597, Con loss: 1.556\n",
      "[77/100] - time: 6.82, Disc loss: 0.067, Gen loss: 0.626, Con loss: 1.558\n",
      "[78/100] - time: 6.83, Disc loss: 0.058, Gen loss: 0.620, Con loss: 1.546\n",
      "[79/100] - time: 6.81, Disc loss: 0.068, Gen loss: 0.617, Con loss: 1.528\n",
      "[80/100] - time: 6.82, Disc loss: 0.067, Gen loss: 0.626, Con loss: 1.523\n",
      "[81/100] - time: 6.83, Disc loss: 0.056, Gen loss: 0.623, Con loss: 1.518\n",
      "[82/100] - time: 6.84, Disc loss: 0.064, Gen loss: 0.631, Con loss: 1.513\n",
      "[83/100] - time: 6.80, Disc loss: 0.056, Gen loss: 0.634, Con loss: 1.504\n",
      "[84/100] - time: 6.81, Disc loss: 0.057, Gen loss: 0.640, Con loss: 1.519\n",
      "[85/100] - time: 6.81, Disc loss: 0.066, Gen loss: 0.640, Con loss: 1.504\n",
      "[86/100] - time: 6.81, Disc loss: 0.069, Gen loss: 0.608, Con loss: 1.501\n",
      "[87/100] - time: 6.83, Disc loss: 0.058, Gen loss: 0.633, Con loss: 1.486\n",
      "[88/100] - time: 6.82, Disc loss: 0.072, Gen loss: 0.604, Con loss: 1.500\n",
      "[89/100] - time: 6.81, Disc loss: 0.064, Gen loss: 0.640, Con loss: 1.487\n",
      "[90/100] - time: 6.83, Disc loss: 0.052, Gen loss: 0.659, Con loss: 1.486\n",
      "[91/100] - time: 6.82, Disc loss: 0.068, Gen loss: 0.634, Con loss: 1.479\n",
      "[92/100] - time: 6.84, Disc loss: 0.062, Gen loss: 0.620, Con loss: 1.487\n",
      "[93/100] - time: 6.82, Disc loss: 0.076, Gen loss: 0.616, Con loss: 1.471\n",
      "[94/100] - time: 6.82, Disc loss: 0.067, Gen loss: 0.599, Con loss: 1.455\n",
      "[95/100] - time: 6.80, Disc loss: 0.056, Gen loss: 0.636, Con loss: 1.461\n",
      "[96/100] - time: 6.81, Disc loss: 0.062, Gen loss: 0.629, Con loss: 1.457\n",
      "[97/100] - time: 6.83, Disc loss: 0.073, Gen loss: 0.620, Con loss: 1.450\n",
      "[98/100] - time: 6.83, Disc loss: 0.065, Gen loss: 0.634, Con loss: 1.455\n",
      "[99/100] - time: 6.82, Disc loss: 0.067, Gen loss: 0.625, Con loss: 1.436\n",
      "[100/100] - time: 6.81, Disc loss: 0.064, Gen loss: 0.641, Con loss: 1.431\n",
      "[101/100] - time: 6.83, Disc loss: 0.037, Gen loss: 0.665, Con loss: 1.426\n",
      "[102/100] - time: 7.05, Disc loss: 0.037, Gen loss: 0.673, Con loss: 1.383\n",
      "[103/100] - time: 7.03, Disc loss: 0.038, Gen loss: 0.674, Con loss: 1.370\n",
      "[104/100] - time: 6.80, Disc loss: 0.037, Gen loss: 0.669, Con loss: 1.368\n",
      "[105/100] - time: 6.83, Disc loss: 0.040, Gen loss: 0.666, Con loss: 1.351\n",
      "[106/100] - time: 6.83, Disc loss: 0.042, Gen loss: 0.660, Con loss: 1.362\n",
      "[107/100] - time: 6.81, Disc loss: 0.043, Gen loss: 0.662, Con loss: 1.366\n",
      "[108/100] - time: 6.83, Disc loss: 0.043, Gen loss: 0.655, Con loss: 1.356\n",
      "[109/100] - time: 6.84, Disc loss: 0.043, Gen loss: 0.653, Con loss: 1.356\n",
      "[110/100] - time: 6.81, Disc loss: 0.045, Gen loss: 0.655, Con loss: 1.351\n",
      "[111/100] - time: 6.80, Disc loss: 0.047, Gen loss: 0.646, Con loss: 1.348\n",
      "[112/100] - time: 6.79, Disc loss: 0.047, Gen loss: 0.640, Con loss: 1.352\n",
      "[113/100] - time: 7.22, Disc loss: 0.048, Gen loss: 0.652, Con loss: 1.354\n",
      "[114/100] - time: 7.45, Disc loss: 0.047, Gen loss: 0.646, Con loss: 1.351\n",
      "[115/100] - time: 6.80, Disc loss: 0.046, Gen loss: 0.643, Con loss: 1.351\n",
      "[116/100] - time: 6.80, Disc loss: 0.047, Gen loss: 0.637, Con loss: 1.358\n",
      "[117/100] - time: 6.83, Disc loss: 0.049, Gen loss: 0.640, Con loss: 1.351\n",
      "[118/100] - time: 6.79, Disc loss: 0.051, Gen loss: 0.643, Con loss: 1.359\n",
      "[119/100] - time: 6.80, Disc loss: 0.047, Gen loss: 0.651, Con loss: 1.358\n",
      "[120/100] - time: 6.81, Disc loss: 0.048, Gen loss: 0.642, Con loss: 1.355\n",
      "[121/100] - time: 6.82, Disc loss: 0.050, Gen loss: 0.640, Con loss: 1.355\n",
      "[122/100] - time: 6.80, Disc loss: 0.047, Gen loss: 0.650, Con loss: 1.348\n",
      "[123/100] - time: 6.81, Disc loss: 0.051, Gen loss: 0.633, Con loss: 1.347\n",
      "[124/100] - time: 6.80, Disc loss: 0.052, Gen loss: 0.635, Con loss: 1.362\n",
      "[125/100] - time: 6.82, Disc loss: 0.050, Gen loss: 0.642, Con loss: 1.368\n",
      "[126/100] - time: 6.80, Disc loss: 0.051, Gen loss: 0.634, Con loss: 1.361\n",
      "[127/100] - time: 6.82, Disc loss: 0.051, Gen loss: 0.638, Con loss: 1.353\n",
      "[128/100] - time: 6.82, Disc loss: 0.054, Gen loss: 0.633, Con loss: 1.360\n",
      "[129/100] - time: 6.81, Disc loss: 0.054, Gen loss: 0.642, Con loss: 1.352\n",
      "[130/100] - time: 6.82, Disc loss: 0.049, Gen loss: 0.637, Con loss: 1.355\n",
      "[131/100] - time: 6.79, Disc loss: 0.052, Gen loss: 0.639, Con loss: 1.352\n",
      "[132/100] - time: 6.79, Disc loss: 0.053, Gen loss: 0.637, Con loss: 1.352\n",
      "[133/100] - time: 6.82, Disc loss: 0.054, Gen loss: 0.633, Con loss: 1.349\n",
      "[134/100] - time: 6.79, Disc loss: 0.054, Gen loss: 0.637, Con loss: 1.342\n",
      "[135/100] - time: 6.79, Disc loss: 0.056, Gen loss: 0.634, Con loss: 1.350\n",
      "[136/100] - time: 6.79, Disc loss: 0.054, Gen loss: 0.635, Con loss: 1.356\n",
      "[137/100] - time: 6.80, Disc loss: 0.055, Gen loss: 0.634, Con loss: 1.354\n",
      "[138/100] - time: 6.80, Disc loss: 0.055, Gen loss: 0.635, Con loss: 1.353\n",
      "[139/100] - time: 6.82, Disc loss: 0.053, Gen loss: 0.636, Con loss: 1.358\n",
      "[140/100] - time: 6.82, Disc loss: 0.059, Gen loss: 0.629, Con loss: 1.353\n",
      "[141/100] - time: 6.83, Disc loss: 0.055, Gen loss: 0.635, Con loss: 1.352\n",
      "[142/100] - time: 6.80, Disc loss: 0.058, Gen loss: 0.628, Con loss: 1.344\n",
      "[143/100] - time: 6.96, Disc loss: 0.055, Gen loss: 0.640, Con loss: 1.357\n",
      "[144/100] - time: 7.13, Disc loss: 0.057, Gen loss: 0.624, Con loss: 1.352\n",
      "[145/100] - time: 6.80, Disc loss: 0.059, Gen loss: 0.630, Con loss: 1.345\n",
      "[146/100] - time: 6.81, Disc loss: 0.058, Gen loss: 0.628, Con loss: 1.346\n",
      "[147/100] - time: 6.80, Disc loss: 0.056, Gen loss: 0.623, Con loss: 1.348\n",
      "[148/100] - time: 6.79, Disc loss: 0.056, Gen loss: 0.638, Con loss: 1.352\n",
      "[149/100] - time: 6.81, Disc loss: 0.057, Gen loss: 0.625, Con loss: 1.341\n",
      "[150/100] - time: 6.78, Disc loss: 0.058, Gen loss: 0.631, Con loss: 1.335\n",
      "[151/100] - time: 6.77, Disc loss: 0.054, Gen loss: 0.631, Con loss: 1.347\n",
      "[152/100] - time: 6.78, Disc loss: 0.050, Gen loss: 0.629, Con loss: 1.350\n",
      "[153/100] - time: 6.81, Disc loss: 0.050, Gen loss: 0.642, Con loss: 1.343\n",
      "[154/100] - time: 6.80, Disc loss: 0.051, Gen loss: 0.634, Con loss: 1.354\n",
      "[155/100] - time: 7.52, Disc loss: 0.051, Gen loss: 0.635, Con loss: 1.341\n",
      "[156/100] - time: 7.14, Disc loss: 0.052, Gen loss: 0.631, Con loss: 1.346\n",
      "[157/100] - time: 6.82, Disc loss: 0.049, Gen loss: 0.639, Con loss: 1.352\n",
      "[158/100] - time: 6.80, Disc loss: 0.055, Gen loss: 0.629, Con loss: 1.345\n",
      "[159/100] - time: 6.80, Disc loss: 0.051, Gen loss: 0.629, Con loss: 1.353\n",
      "[160/100] - time: 6.79, Disc loss: 0.053, Gen loss: 0.632, Con loss: 1.345\n",
      "[161/100] - time: 6.80, Disc loss: 0.052, Gen loss: 0.628, Con loss: 1.344\n",
      "[162/100] - time: 6.82, Disc loss: 0.054, Gen loss: 0.629, Con loss: 1.346\n",
      "[163/100] - time: 6.82, Disc loss: 0.052, Gen loss: 0.640, Con loss: 1.347\n",
      "[164/100] - time: 6.79, Disc loss: 0.051, Gen loss: 0.633, Con loss: 1.337\n",
      "[165/100] - time: 6.79, Disc loss: 0.051, Gen loss: 0.631, Con loss: 1.350\n",
      "[166/100] - time: 6.82, Disc loss: 0.052, Gen loss: 0.633, Con loss: 1.341\n",
      "[167/100] - time: 6.81, Disc loss: 0.054, Gen loss: 0.628, Con loss: 1.343\n"
     ]
    }
   ],
   "source": [
    "print('training start!')\n",
    "start_time = time.time()\n",
    "num_pool = 50\n",
    "fake_pool = utils.ImagePool(num_pool)\n",
    "# real = torch.ones(batch_size, 1, input_size // 4, input_size // 4).to(device)\n",
    "# fake = torch.zeros(batch_size, 1, input_size // 4, input_size // 4).to(device)\n",
    "for epoch in range(train_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    G.train()\n",
    "    G_scheduler.step()\n",
    "    D_scheduler.step()\n",
    "    Disc_losses = []\n",
    "    Gen_losses = []\n",
    "    Con_losses = []\n",
    "    for d, _ in train_loader:\n",
    "        # x is the image at left, the source image\n",
    "        # y is the image at right, the target image\n",
    "        y = d[:, :, :, :input_size]\n",
    "        x = d[:, :, :, input_size:]\n",
    "        x, y, d = x.to(device), y.to(device), d.to(device)\n",
    "        \n",
    "        # train D\n",
    "        for param in D.parameters():\n",
    "            param.requires_grad = True\n",
    "#             param.data.clamp_(-0.005,0.005)\n",
    "        D_optimizer.zero_grad()\n",
    "\n",
    "        real = torch.cat((y,x),1)\n",
    "        D_real = D(real)\n",
    "#         D_real_loss = D_loss_criterion(D_real)\n",
    "        D_real_loss = GAN_loss(D_real, 1-torch.rand(D_real.size(),device = device)/10.0)\n",
    "#         D_real_loss = GAN_loss(D_real, torch.ones(D_real.size(),device = device))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            G_ = G(x)\n",
    "        generated = torch.cat((G_,x), 1)\n",
    "        generated = fake_pool.query(generated.detach())\n",
    "        D_fake = D(generated)\n",
    "#         D_fake_loss = D_loss_criterion(D_fake)\n",
    "        D_fake_loss = GAN_loss(D_fake, torch.rand(D_fake.size(),device = device)/10.0)\n",
    "#         D_fake_loss = GAN_loss(D_fake, torch.zeros(D_fake.size(),device = device))\n",
    "\n",
    "        Disc_loss = 0.5 * (D_real_loss + D_fake_loss)\n",
    "#         Disc_loss = D_fake_loss - D_real_loss\n",
    "        Disc_losses.append(Disc_loss.item())\n",
    "        train_hist['Disc_loss'].append(Disc_loss.item())\n",
    "        Disc_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "        # train G\n",
    "        for param in D.parameters():\n",
    "            param.requires_grad = False\n",
    "        G_optimizer.zero_grad()\n",
    "\n",
    "        G_ = G(x)\n",
    "        generated = torch.cat((G_,x), 1)\n",
    "        D_fake = D(generated)\n",
    "#         D_fake_loss = D_loss_criterion(D_fake)\n",
    "        D_fake_loss = GAN_loss(D_fake, torch.ones(D_fake.size(),device = device))\n",
    "\n",
    "        Con_loss = con_lambda * L1_loss(G_, y)\n",
    "\n",
    "        Gen_loss = D_fake_loss + Con_loss\n",
    "#         Gen_loss = -D_fake_loss + Con_loss\n",
    "\n",
    "        Gen_losses.append(D_fake_loss.item())\n",
    "        train_hist['Gen_loss'].append(D_fake_loss.item())\n",
    "        Con_losses.append(Con_loss.item())\n",
    "        train_hist['Con_loss'].append(Con_loss.item())\n",
    "\n",
    "        Gen_loss.backward()\n",
    "        G_optimizer.step()\n",
    "\n",
    "\n",
    "    per_epoch_time = time.time() - epoch_start_time\n",
    "    train_hist['per_epoch_time'].append(per_epoch_time)\n",
    "    \n",
    "    Gen_loss_avg = torch.mean(torch.FloatTensor(Gen_losses))\n",
    "    Con_loss_avg = torch.mean(torch.FloatTensor(Con_losses))\n",
    "    Disc_loss_avg =  torch.mean(torch.FloatTensor(Disc_losses))\n",
    "    \n",
    "    train_hist['Gen_loss_one_epoch'].append(Gen_loss_avg)\n",
    "    train_hist['Disc_loss_one_epoch'].append(Disc_loss_avg)\n",
    "    train_hist['Con_loss_one_epoch'].append(Con_loss_avg)\n",
    "    \n",
    "    print(\n",
    "    '[%d/%d] - time: %.2f, Disc loss: %.3f, Gen loss: %.3f, Con loss: %.3f' % ((starting_epoch + epoch + 1), train_epoch, per_epoch_time, Disc_loss_avg, Gen_loss_avg, Con_loss_avg))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        G.eval()\n",
    "        for n, (d, _) in enumerate(train_loader):\n",
    "            y = d[:, :, :, :input_size]\n",
    "            x = d[:, :, :, input_size:]            \n",
    "            x,y = x.to(device),y.to(device)\n",
    "            G_recon = G(x)\n",
    "            result = torch.cat((x[0], G_recon[0],y[0]), 2)\n",
    "            path = os.path.join(result_path, str(starting_epoch+epoch+1) + '_epoch_' + project_name + '_train_' + str(n + 1) + '.png')\n",
    "            plt.imsave(path, (result.cpu().numpy().transpose(1, 2, 0) + 1) / 2)\n",
    "            if n == 2:\n",
    "                break\n",
    "        for n, (d, _) in enumerate(test_loader):\n",
    "            y = d[:, :, :, :input_size]\n",
    "            x = d[:, :, :, input_size:]            \n",
    "            x,y = x.to(device),y.to(device)\n",
    "            G_recon = G(x)\n",
    "            result = torch.cat((x[0], G_recon[0],y[0]), 2)\n",
    "            path = os.path.join(result_path, str(starting_epoch+epoch+1) + '_epoch_' + project_name + '_test_' + str(n + 1) + '.png')\n",
    "            plt.imsave(path, (result.cpu().numpy().transpose(1, 2, 0) + 1) / 2)\n",
    "            if n == 1:\n",
    "                break\n",
    "                \n",
    "        torch.save(G.state_dict(), os.path.join(result_path, 'generator_latest.pkl'))\n",
    "        torch.save(D.state_dict(), os.path.join(result_path, 'discriminator_latest.pkl'))\n",
    "        with open(os.path.join(result_path,  'train_hist.pkl'), 'wb') as f:\n",
    "            pickle.dump(train_hist, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "        transforms.Resize((input_size, 2*input_size)),\n",
    "        transforms.ColorJitter(0.1,0.1,0.1,0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(datasets.ImageFolder('data/eye_test', train_transform), batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "test_result_path = 'pix2pix_eyes_test_results'\n",
    "\n",
    "# results save path\n",
    "if not os.path.isdir(test_result_path):\n",
    "    os.makedirs(test_result_path)\n",
    "\n",
    "with torch.no_grad():\n",
    "    G.eval()\n",
    "    for n, (d, _) in enumerate(test_loader):\n",
    "#         x = d[:, :, :, :input_size]\n",
    "#         y = d[:, :, :, input_size:]            \n",
    "#         x = x.to(device)\n",
    "#         y = y.to(device)\n",
    "        d = d.to(device)\n",
    "        G_recon = G(d)\n",
    "        result = torch.cat((d[0], G_recon[0]), 2)\n",
    "        path = os.path.join(test_result_path, str(starting_epoch+epoch+1) + '_epoch_' + project_name + '_test_' + str(n + 1) + '.png')\n",
    "        plt.imsave(path, (result.cpu().numpy().transpose(1, 2, 0) + 1) / 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
